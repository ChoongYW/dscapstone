install.packages("R.utils")
library(R.utils)
countLines(twitter.txt)
x <- "I said,hey ,'tisn't 8 fun?!nope..(ha)"
myRemovePunct <- function(x) {
# replace everything that is not alphanumeric, space, or hyphen with a space
gsub("[^[:alnum:][:space:]']", " ", x)
}
myRemovePunct(x)
x <- "I said,hey ,'tisn't #8@& $5%*\m/ fun?!nope..(ha)"
x <- "I said,hey ,'tisn't #8@& $5%*\\m/ fun?!nope..(ha)"
x
myRemovePunct(x)
myRemovePunct <- function(x) {
# replace everything that is not alphanumeric, space, or hyphen with a space
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
x <- "I said,hey ,'tisn't -#8@& $5%*\\m/ f-un?!nope..(ha)"
myRemovePunct(x)
y <- "this is my link http://www.com yes"
removeURL <- function(x) {
gsub("http[[:alnum:]]", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeMostPunct <- function(x) {
# replace everything that isn't alphanumeric, space, apostrophe or hyphen with a space
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
removeURL(removeMostPunct(y))
removeURL <- function(x) {
gsub("http.*?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http://.*?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http://[:alnum:]+*?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http://[:alnum:]+?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
y
removeURL <- function(x) {
gsub("http.+?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http.*", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
y <- "this is my link http://ww.com and http:/yes.org"
removeURL(y)
removeURL <- function(x) {
gsub("http.*?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http[:/]*.*?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http[:/]*.*", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
y <- "this is my link http://ww.com and http:/yes.org and here"
removeURL(u)
removeURL(y)
removeURL <- function(x) {
gsub("http[:/]*.+ ", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
y
gsub("http.*? ", "", y)
x <- "this is my link http://ww.com and http:/yes.org"
gsub("http.*? ", "", x)
gsub("http.*?[ $]", "", x)
gsub("http.*?( |$)", "", x)
gsub("http.*?[ $]", "", y)
x
y
gsub("http.*?[ $]", "", x)
gsub("http.*?( |$)", "", x)
gsub("http.*?( |$)", "", y)
a <- this is my link http://ww.com, and http:/yes.org. and here
a <- "this is my link http://ww.com, and http:/yes.org. and here"
gsub("http.*?( |$)", "", a)
a <- "this is my link http://ww.com , and http:/yes.org). and here"
gsub("http.*?( |$)", "", a)
x
gsub("http.*?( |$)", "", x)
x <- "My sister-in-law came--today---and -I- am a-okay!"
gsub("--+", "", x)
gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
x <- "#hi My sister-in-law came--today---and -I- am a-okay!@home* (~ha)?"
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
handleDashApost <- function(x) {
x <- gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
}
handleDashApost(x)
x
handleDashApost(removeMostPunct(x))
x <- "#hi My sister-in-law came--today---and -I- isn't 'twas am a-okay!@home* (~ha)?"
handleDashApost(removeMostPunct(x))
stripWhitespace(handleDashApost(removeMostPunct(x)))
library(tm)
stripWhitespace(handleDashApost(removeMostPunct(x)))
x <- "'sup' #hi My sister-in-law came--today---and -I- isn't 'twas am a-okay!@home* (~ha)?\"yo\""
stripWhitespace(handleDashApost(removeMostPunct(x)))
trimWhitespace <- function(x) {
gsub("^\\s+|\\s+$", "", x)
}
trimWhitespace(stripWhitespace(handleDashApost(removeMostPunct(x))))
blog.mini <- readLines("./blog.sample.txt")  # imports as character vector
news.mini <- readLines("./news.sample.txt")
twit.mini <- readLines("./twit.sample.txt")
setwd("dscapstone")
system("ls")
blog.mini <- readLines("./blog.sample.txt")  # imports as character vector
news.mini <- readLines("./news.sample.txt")
twit.mini <- readLines("./twit.sample.txt")
library(tm)
# build a corpus, specifying the vector source as a character vector
blog.corpus <- Corpus(VectorSource(blog.mini))
inspect(blog.corpus[20])
blog.corpus <- tm_map(blog.corpus, content_transformer(tolower))
inspect(blog.corpus[20])
blog.corpus <- tm_map(blog.corpus, content_transformer(removeNumbers))
inspect(blog.corpus[20])
x <- "1st and 4most my b2b wd40 is 24/7 3lb 4 kg"
gsub("[[:digit:]]"," ",x)
gsub("[[:digit:]]+"," ",x)
gsub("[[:digit:]].+"," ",x)
gsub("[[:digit:]].*?"," ",x)
gsub("\w\d\w", " ",x)
gsub(".*?[0-9]+.*?", " ",x)
x
gsub("[^\s]*[0-9]+[^\s]*", " ", x)
gsub("\\S+[0-9]+\\S+", " ", x)
gsub("\\S*[0-9]+\\S+", " ", x)
gsub("\\S*[0-9]+\\S*", " ", x)
gsub("\\S*[0-9]+\\S*", "", x)
x <- "1st, and .4most my b2b wd40 is 24/7 ! 3lb! 4 kg?"
gsub("\\S*[0-9]+\\S*", "", x)
inspect(blog.corpus[20])
blog.corpus <- Corpus(VectorSource(blog.mini))
blog.corpus <- tm_map(blog.corpus, content_transformer(tolower))
# remove URLs within string and at end of string
removeURL <- function(x) {
gsub("http.*?( |$)", "", x)  # won't work with shortened URLs e.g. bit.ly
}
blog.corpus <- tm_map(blog.corpus, content_transformer(removeURL))
inspect(blog.corpus[20])
myRemoveNumbers <- function(x) {
gsub("\\S*[0-9]+\\S*", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemoveNumbers))
inspect(blog.corpus[20])
# remove punctuation. custom, since removePunctuation() removes too much
myRemovePunctuation <- function(x) {
# replace everything that isn't alphanumeric, space, apostrophe or hyphen
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemovePunctuation))
inspect(blog.corpus[20])
myDashApos <- function(x) {
x <- gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myDashApos))
inspect(blog.corpus[20])
blog.corpus <- tm_map(blog.corpus, content_transformer(stripWhitespace))
# trim leading and trailing whitespace
trim <- function(x) {
gsub("^\\s+|\\s+$", "", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(trim))
inspect(blog.corpus[20])
inspect(blog.corpus[22])
x <- "I passed Michael’s five-year old sister"
gsub("[\u2018\u2019\u201A\u201B\u2032\u2035]", "'", x)
gsub("[\x82\x91\x92]", "'", x)
blog.corpus <- Corpus(VectorSource(blog.mini))
# convert text to lowercase
blog.corpus <- tm_map(blog.corpus, content_transformer(tolower))
# remove URLs within string and at end of string
removeURL <- function(x) {
gsub("http.*?( |$)", "", x)  # won't work with shortened URLs e.g. bit.ly
}
blog.corpus <- tm_map(blog.corpus, content_transformer(removeURL))
myRemoveNumbers <- function(x) {
gsub("\\S*[0-9]+\\S*", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemoveNumbers))
# remove punctuation. custom, since removePunctuation() removes too much
myRemovePunctuation <- function(x) {
# replace everything that isn't alphanumeric, space, apostrophe or hyphen
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemovePunctuation))
myDashApos <- function(x) {
# replace smart single quotes and apostrophes with straight single quotes
x <- gsub("[\x82\x91\x92]", "'", x)  # ANSI version, not Unicode version
x <- gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myDashApos))
# strip extra whitespace
blog.corpus <- tm_map(blog.corpus, content_transformer(stripWhitespace))
# trim leading and trailing whitespace
trim <- function(x) {
gsub("^\\s+|\\s+$", "", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(trim))
inspect(blog.corpus[20])
inspect(blog.corpus[22])
blog.corpus <- Corpus(VectorSource(blog.mini))
# convert text to lowercase
blog.corpus <- tm_map(blog.corpus, content_transformer(tolower))
# remove URLs within string and at end of string
removeURL <- function(x) {
gsub("http.*?( |$)", "", x)  # won't work with shortened URLs e.g. bit.ly
}
blog.corpus <- tm_map(blog.corpus, content_transformer(removeURL))
# remove any word starting with numbers
myRemoveNumbers <- function(x) {
gsub("\\S*[0-9]+\\S*", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemoveNumbers))
# convert smart single quotes, like ’, to straight single quotes, like '
mySingleQuote <- function(x) {
gsub("[\x82\x91\x92]", "'", x)  # ANSI version, not Unicode version
}
blog.corpus <- tm_map(blog.corpus, content_transformer(mySingleQuote))
# remove punctuation. custom, since removePunctuation() removes too much
myRemovePunctuation <- function(x) {
# replace everything that isn't alphanumeric, space, apostrophe or hyphen
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemovePunctuation))
# deal with dashes and apostrophes within words
myDashApos <- function(x) {
x <- gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myDashApos))
# strip extra whitespace
blog.corpus <- tm_map(blog.corpus, content_transformer(stripWhitespace))
# trim leading and trailing whitespace
trim <- function(x) {
gsub("^\\s+|\\s+$", "", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(trim))
inspect(blog.corpus[22])
getAnywhere(removePunctuation)
getMethod("removePunctuation")
inspect(blog.corpus[22])
blog.tdm <- TermDocumentMatrix(blog.corpus)
inspect(blog.tdm[1:20, 1:5])
blog.tdm
i <- which(dimnames(blog.tdm)$Terms == "winter")
i <- which(dimnames(blog.tdm)$Terms == "winter")
i
inspect(blog.tdm[i+(0:5), 1:5])
findFreqTerms(x=blog.tdm, lowfreq=1000, highfreq=Inf)
findFreqTerms(x=blog.tdm, lowfreq=10000, highfreq=Inf)
dimnames(tdm)
dimnames(blog.tdm)
dimnames(blog.tdm)$Terms[1:3]
blog.tdm
dimnames(blog.tdm)$Terms[69257]
findFreqTerms(blog.tdm, highfreq=1)
findFreqTerms(blog.tdm, lowfreq=0, highfreq=1)[1:5]
findFreqTerms(blog.tdm, lowfreq=0, highfreq=1)[1000:1005]
nrow(findFreqTerms(blog.tdm, lowfreq=0, highfreq=1))
once <- findFreqTerms(blog.tdm, lowfreq=0, highfreq=1)
dim(once)
str(once)
class(once)
length(once)
sample(once,5)
blog.freq
blog.freq <- rowSums(as.matrix(blog.tdm))
bdf <- as.data.frame(blog.tdm)
bm <- as.matrix(blog.tdm)
blog.tdm
bm <- inspect(blog.tdm)
findFreqTerms(x=blog.tdm, lowfreq=10000, highfreq=Inf)
install.packages("slam")
library(slam)
blog.freq <- row_sums(blog.tdm, na.rm=TRUE)
str(blog.freq)
hist(blog.freq)
max(blog.freq)
head(blog.freq)
blog.tdm.denser <- removeSparseTerms(blog.tdm, 0.9)
hist(row_sums(blog.tdm.denser, na.rm=T))
length(stopwords("english"))
head(stopwords("english"))
"i" %in% stopwords("english")
"you" %in% stopwords("english")
"and" %in% stopwords("english")
"but" %in% stopwords("english")
stopwords("english")
findAssocs(blog.tdm, "winter", 0.5)
findAssocs(blog.tdm, "winter", 0.1)
findAssocs(blog.tdm, "cold", 0.1)
findAssocs(blog.tdm, "snow", 0.1)
install.packages("RWeka")
x <- c(1,2,3)
y1 <- c("a","b","c")
ya
y1
paste(y1)
z <- paste(y1)
class(z)
class(y1)
str(y1)
y2 <- y1
y3 <- y1
y1
y2
y3
y <- c(y1,y2,y3)
y
str(y)
rbind(x,y)
y <- c(paste(y1),paste(y2),paste(y3))
rbind(x,y)
y <- c(paste0(y1),paste0(y2),paste0(y3))
rbind(x,y)
y <- c(paste(y1,collapse=""),paste(y2,collapse=""),paste(y3,collapse=""))
rbind(x,y)
y <- c(paste(y1,collapse=" "),paste(y2,collapse=" "),paste(y3,collapse=" "))
rbind(x,y)
z <- data.frame(rbind(x,y))
row.names(z)
row.names(z) <- c("xx","yy")
z
blog.mini <- readLines("./blog.sample.txt")  # imports txt as character vector
news.mini <- readLines("./news.sample.txt")
twit.mini <- readLines("./twit.sample.txt")
library(tm)
# build a corpus, from a character vector
blog.corpus.raw <- Corpus(VectorSource(blog.mini))
news.corpus.raw <- Corpus(VectorSource(news.mini))
twit.corpus.raw <- Corpus(VectorSource(twit.mini))
inspect(blog.corpus.raw[22])
inspect(news.corpus.raw[22])
inspect(news.corpus.raw[23])
inspect(news.corpus.raw[25])
inspect(twit.corpus.raw[25])
cleanCorpus <- function(my.corpus) {
# 1. convert text to lowercase
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
# 2. remove URLs within string and at end of string
removeURL <- function(x) {
gsub("http.*?( |$)", "", x)  # won't work with shortened URLs e.g. bit.ly
}
my.corpus <- tm_map(my.corpus, content_transformer(removeURL))
# 3. remove any word containing numbers
myRemoveNumbers <- function(x) {
gsub("\\S*[0-9]+\\S*", " ", x)
}
my.corpus <- tm_map(my.corpus, content_transformer(myRemoveNumbers))
# 4. convert smart single quotes to straight single quotes
mySingleQuote <- function(x) {
gsub("[\x82\x91\x92]", "'", x)  # ANSI version, not Unicode version
}
my.corpus <- tm_map(my.corpus, content_transformer(mySingleQuote))
# 5. custom function to remove most punctuation
myRemovePunctuation <- function(x) {
# replace everything that isn't alphanumeric, space, apostrophe or hyphen
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
my.corpus <- tm_map(my.corpus, content_transformer(myRemovePunctuation))
# 6. deal with dashes and apostrophes within words
myDashApos <- function(x) {
x <- gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
}
my.corpus <- tm_map(my.corpus, content_transformer(myDashApos))
# remove stopwords - optional
# blog.corpus <- tm_map(blog.corpus, removeWords, stopwords("english"))
# 7. strip extra whitespace
my.corpus <- tm_map(my.corpus, content_transformer(stripWhitespace))
# 8. trim leading and trailing whitespace
trim <- function(x) {
gsub("^\\s+|\\s+$", "", x)
}
my.corpus <- tm_map(my.corpus, content_transformer(trim))
return(my.corpus)
}
blog.corpus <- cleanCorpus(blog.corpus.raw)
news.corpus <- cleanCorpus(news.corpus.raw)
twit.corpus <- cleanCorpus(twit.corpus.raw)
inspect(blog.corpus[22])
inspect(blog.corpus[1])
inspect(blog.corpus[42])
inspect(twit.corpus[44])
blog.tdm <- TermDocumentMatrix(blog.corpus)
news.tdm <- TermDocumentMatrix(news.corpus)
twit.tdm <- TermDocumentMatrix(twit.corpus)
blog.tdm
news.tdm
twit.tdm
i <- which(dimnames(blog.tdm)$Terms == "winter")
inspect(blog.tdm[i+(0:5), 1:10])
library(slam)
blog.freq <- row_sums(blog.tdm, na.rm=TRUE)
news.freq <- row_sums(news.tdm, na.rm=TRUE)
twit.freq <- row_sums(twit.tdm, na.rm=TRUE)
par(mfrow=c(1,3))  # fit graphs into 1 row, 3 cols
hist(blog.freq)
hist(news.freq)
hist(twit.freq)
blog.once <- findFreqTerms(blog.tdm, lowfreq=0, highfreq=1)
news.once <- findFreqTerms(news.tdm, lowfreq=0, highfreq=1)
twit.once <- findFreqTerms(twit.tdm, lowfreq=0, highfreq=1)
# number of terms that appear at most one time
num.once <- c(length(blog.once), length(news.once), length(twit.once))
# randomly sample 3 of these words
set.seed(773)
ex.once <- c(paste(sample(blog.once, 3), collapse=" "),
paste(sample(news.once, 3), collapse=" "),
paste(sample(twit.once, 3), collapse=" "))
df.once <- data.frame(rbind(num.once, ex.once))
row.names(df.once) <- c("Number of words that appear only once",
"Examples of such words in the text")
col.names(df.once) <- c("blog", "news", "Twitter")
colnames(df.once) <- c("blog", "news", "Twitter")
rownames(df.once) <- c("Number of words that appear only once",
"Examples of such words in the text")
colnames(df.once) <- c("blog", "news", "Twitter")
df.once
x <- c(1,2,3)
y <- c("a","b","c")
cbind(x,y)
df.once <- data.frame(cbind(num.once, ex.once))
colnames(df.once) <- c("No. of words that appear only once",
"Examples of such words in the text")
rownames(df.once) <- c("blog", "news", "Twitter")
df.once
length("Men set around 2gether talking about what they would do to all these hot women they see daily. Women set around talking about the men they..")
tw <- "Perfect weekend in Plano with ! Chilis for lunch with and now what to do??#donewithfinals #summertime <f0><U+009F><U+0092><U+009B><f0><U+009F><U+0092><U+008B><f0><U+009F><U+0091><U+0099><f0><U+009F><U+0092><U+0098><U+2600><f0><U+009F><U+0092><U+0090><f0><U+009F><U+0098><U+008A>"
gsub("<.+>","", tw)
tw <- "Perfect weekend in Plano with ! Chilis for lunch with and now what to do??#donewithfinals #summertime <f0><U+009F><U+0092><U+009B><f0><U+009F><U+0092><U+008B><f0><U+009F><U+0091><U+0099><f0><U+009F><U+0092><U+0098><U+2600><f0><U+009F><U+0092><U+0090><f0><U+009F><U+0098><U+008A> hi"
gsub("<.+>","", tw)
gsub("^<.+>$","", tw)
gsub("<.+>","", tw)
x <- "test <U+008A>hi<f0><U+008A><U+008A> test <U+008A>this"
gsub('<.+>',' ',x)
gsub('<.+?>',' ',x)
system("awk 'length > max_length { max_length = length; longest_line = $0 } END { print longest_line }' twit.sample.txt")
```{r ref.label=1, echo=TRUE, eval=FALSE}
# 6. deal with dashes, apostrophes, asterisks within words
source('C:/Code/Coursera/capstone/dscapstone/ngram.R')
source('C:/Code/Coursera/capstone/dscapstone/ngram.R')
save.image("C:/Code/Coursera/capstone/dscapstone/start_here.RData")
inspect(twit.tdm.dense[1:5,1:5])
inspect(twit.tdm.dense[1,1])
twit.tdm
inspect(twit.tdm[1,1])
inspect(twit.tdm[1:10,1])
inspect(twit.tdm[10:20,1])
inspect(twit.tdm[20:30,1])
inspect(twit.tdm[30:40,1])
inspect(twit.tdm[40:50,1])
inspect(twit.tdm[50:60,1])
inspect(twit.tdm[60:70,1])
inspect(twit.tdm[70:80,1])
inspect(twit.tdm[80:90,1])
inspect(blog.tdm[80:90,1])
i <- which(dimnames(blog.tdm)$Terms == "organize")
inspect(blog.tdm[i+(0:10), 1:10])
i <- which(dimnames(twit.tdm)$Terms == "organize")
inspect(twit.tdm[i+(0:10), 1:10])
inspect(blog.tdm2[1,1])
i <- which(dimnames(blog.tdm)$Terms == "a a")
inspect(twit.tdm[i+(0:10), 1:20])
i <- which(dimnames(blog.tdm)$Terms == "a a")
inspect(blog.tdm[i+(0:10), 1:20])
find.packages("installr")
find.package("installr")
library(installr)
updateR()
updateR()
library(installr)
updateR()
updateR()
library(tm)
runif()
runif(1)
runif(1)
runif(2)
library(caret)
runif(1) * 100
runif(1) * 100
runif(1) * 100
runif(1) * 100
runif(1) * 100
runif(1) * 100
runif(1) * 100
runif(1) * 100
runif(1) * 100
hist(runif(100))
hist(runif(1000))
