---
title: "Swiftkey capstone milestone report"
author: "Melissa Tan"
date: "Sunday, March 22, 2015"
output:
  html_document:
    keep_md: yes
  pdf_document: default  
---

```{r setoptions, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
library(knitr)
opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE)
```

## Introduction

Given texts from blogs, news sites and Twitter that were collected from publicly available sources by a web crawler, I analyze the corpora to identify major features. The corpora will later be used to train an algorithm. The goals for the algorithm and eventual app are to predict the next word that will appear after a given phrase. This milestone report focuses on the exploratory analysis.

## Download data

As instructed, the dataset must be downloaded from the link given in the Coursera website, [https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). Unzip into parent directory.

```{r unzip}
if (!file.exists("../final")) {
  fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
  download.file(fileUrl, destfile = "../swiftkey.zip")
  unzip("../swiftkey.zip")
}
```

Unzipping gives us a directory called `final`. The datasets that we will use are inside the subdirectory `en_US`. Print them out using system call.

```{r ls}
orig.wd <- getwd() # save working dir, to return to later
setwd("../final/en_US")
system("ls")
setwd(orig.wd)
```

We will be using these three datasets for text prediction:     
* en_US.blogs.txt - text from blog posts
* en_US.news.txt - text from 
* en_US.twitter.txt

## Basic summary of data

Here's a basic summary of each of the three datasets. For this section I'll mostly be using system tools (Unix commands), since that's quick and straightforward.

### Word and line counts: 

Word count: Use Unix command `wc`, with `-w` flag.    
Line count: Use `wc`, with `-l` flag.   
Length of longest line (in terms of word count): Use `wc`, with `-L` flag.  

```{r wc_summary}
setwd("../final/en_US")
numwords <- system("wc -w *.txt", intern=TRUE)  # intern=TRUE to return output  
numlines <- system("wc -l *.txt", intern=TRUE)
longest <- system("wc -L *.txt", intern=TRUE)
setwd(orig.wd)  # return to original working dir, ie. the parent of /final

# number of words for each dataset
blog.numwords <- as.numeric(gsub('[^0-9]', '', numwords[1]))
news.numwords <- as.numeric(gsub('[^0-9]', '', numwords[2]))
twit.numwords <- as.numeric(gsub('[^0-9]', '', numwords[3]))
# number of lines for each dataset
blog.numlines <- as.numeric(gsub('[^0-9]', '', numlines[1]))
news.numlines <- as.numeric(gsub('[^0-9]', '', numlines[2]))
twit.numlines <- as.numeric(gsub('[^0-9]', '', numlines[3]))
# length of longest line for each dataset
blog.longest  <- as.numeric(gsub('[^0-9]', '', longest[1]))
news.longest  <- as.numeric(gsub('[^0-9]', '', longest[2]))
twit.longest  <- as.numeric(gsub('[^0-9]', '', longest[3]))

# create and display summary table
blog.stats <- c(blog.numwords, blog.numlines, blog.longest,
                round(blog.numwords/blog.numlines))
news.stats <- c(news.numwords, news.numlines, news.longest,
                round(news.numwords/news.numlines))
twit.stats <- c(twit.numwords, twit.numlines, twit.longest, 
                round(twit.numwords/twit.numlines))  
data.stats <- data.frame(rbind(blog.stats, news.stats, twit.stats))
names(data.stats) <- c("Total word count", 
                       "Total line count", 
                       "Words in longest line",
                       "Avg words per line")
kable(data.stats)  # display the above in table format
```

## Take random subsample

Since the datasets are too large for my laptop memory to handle, I write a function that will produce a random subsample of each of the datasets. The function essentially flips a coin to decide whether to include a particular line from the source text in the subsample. It then writes the subsample as a txt file, for convenience and reproducibility. Each subsample can be further split later on into training, validation and testing sets. 

```{r fn_sample}
## Function to create subsample of txt file 
SampleTxt <- function(infile, outfile, seed, inlines, percent, readmode) {
  conn.in <- file(infile, readmode)  # readmode = "r" or "rb"
  conn.out <- file(outfile,"w")  
  # for each line, flip a coin to decide whether to put it in sample
  set.seed(seed)
  in.sample <- rbinom(n=inlines, size=1, prob=percent)
  i <- 0
  num.out <- 0  # number of lines written out to subsample
  for (i in 1:(inlines+1)) {
    # read in one line at a time
    currLine <- readLines(conn.in, n=1, encoding="UTF-8", skipNul=TRUE) 
    # if reached end of file, close all conns
    if (length(currLine) == 0) {  
      close(conn.out)  
      close(conn.in)
      # return number of lines written out to subsample 
      return(num.out)  
    }  
    # while not end of file, write out the selected line to file
    if (in.sample[i] == 1) {
      writeLines(currLine, conn.out)
      num.out <- num.out + 1
    }
  }
}
```

I extract about 5% of the original source text for each subsample. The subsample files are saved in the current working directory.

```{r mk_sample}
datalist <- c("../final/en_US/en_US.blogs.txt",
              "../final/en_US/en_US.news.txt",
              "../final/en_US/en_US.twitter.txt")
mypercent <- 0.05
myseed <- 60637

if (!file.exists("./blog.sample.txt")) {
  SampleTxt(datalist[1], "blog.sample.txt", myseed, blog.numlines, mypercent, "r")
}
if (!file.exists("./news.sample.txt")) {
  # need to use read mode rb for this because otherwise it breaks
  SampleTxt(datalist[2], "news.sample.txt", myseed, news.numlines, mypercent, "rb")
}
if (!file.exists("./twit.sample.txt")) {
  SampleTxt(datalist[3], "twit.sample.txt", myseed, twit.numlines, mypercent, "r")
}
```

## Basic summary of subsample

Compute word and line statistics to see how the subsamples compare with the source text.

```{r sample_wc}
sample.numwords <- system("wc -w *.sample.txt", intern=TRUE)  
sample.numlines <- system("wc -l *.sample.txt", intern=TRUE)
sample.longest <- system("wc -L *.sample.txt", intern=TRUE)

# number of words for each dataset
blog.sample.numwords <- as.numeric(gsub('[^0-9]', '', sample.numwords[1]))
news.sample.numwords <- as.numeric(gsub('[^0-9]', '', sample.numwords[2]))
twit.sample.numwords <- as.numeric(gsub('[^0-9]', '', sample.numwords[3]))
# number of lines for each dataset
blog.sample.numlines <- as.numeric(gsub('[^0-9]', '', sample.numlines[1]))
news.sample.numlines <- as.numeric(gsub('[^0-9]', '', sample.numlines[2]))
twit.sample.numlines <- as.numeric(gsub('[^0-9]', '', sample.numlines[3]))
# length of longest line for each dataset
blog.sample.longest  <- as.numeric(gsub('[^0-9]', '',  sample.longest[1]))
news.sample.longest  <- as.numeric(gsub('[^0-9]', '',  sample.longest[2]))
twit.sample.longest  <- as.numeric(gsub('[^0-9]', '',  sample.longest[3]))

# create and display summary table
blog.sample.stats <- c(blog.sample.numwords, blog.sample.numlines, blog.sample.longest,
                      round(blog.sample.numwords/blog.sample.numlines))
news.sample.stats <- c(news.sample.numwords, news.sample.numlines, news.sample.longest,
                      round(news.sample.numwords/news.sample.numlines))
twit.sample.stats <- c(twit.sample.numwords, twit.sample.numlines, twit.sample.longest,
                      round(twit.sample.numwords/twit.sample.numlines))  
sample.stats <- data.frame(rbind(blog.sample.stats, 
                                 news.sample.stats, 
                                 twit.sample.stats))
names(sample.stats) <- c("Sample word count", 
                       "Sample line count", 
                       "Words in longest line", 
                       "Avg words per line")
kable(sample.stats)  # display the above in table format
```

## Import and clean subsample

Read in each of the subsample txts.

```{r readsubsample}
blog.mini <- readLines("./blog.sample.txt")  # imports as character vector
news.mini <- readLines("./news.sample.txt")
twit.mini <- readLines("./twit.sample.txt")
```

## Visualize frequent words in subsample

Use the `tm` text mining package in R to clean and analyze the subsample. 

First, convert text to lowercase, and remove URLs.

```{r corpus}
library(tm)
# build a corpus, specifying the vector source as a character vector
blog.corpus <- Corpus(VectorSource(blog.mini))

# convert text to lowercase
blog.corpus <- tm_map(blog.corpus, content_transformer(tolower))

# remove URLs within string and at end of string
removeURL <- function(x) {
  gsub("http.*?( |$)", "", x)  # won't work with shortened URLs e.g. bit.ly
}
blog.corpus <- tm_map(blog.corpus, content_transformer(removeURL))
```

At this juncture, I have to make a few decisions about how to handle the following cases that may appear in the text.

* Words containing numbers, e.g. "007", "1st", "b2c", "d20", "24/7". I choose to remove all of these.

* Smart quotes, e.g. `'`. I choose to convert all to single quotes e.g. `'`.

* Punctuation: there is a standard `removePunctuation()` function in the `tm` package. However, it removes the entire `[:punct:]` POSIX class, including hyphens and single quotes or apostrophes. I find this to be a little excessive, because I still want to keep the intra-word occurrences of `-` and `'`, e.g. `addle-pated`, `mother-in-law`, `isn't`. So I write my own functions to remove most punctuation except `-` and `'`, and then handle those two marks separately.

* Hyphens: It's probably easier to illustrate with an example. For a sentence such as `my mother-in-law visited--what joy-- i was absolutely -thrilled-!`, I want to convert that to `my mother-in-law visited what joy i was absolutely thrilled!`

* Single quotes and apostrophes: I choose to keep intra-word apostrophes. Though I would like to keep leading apostrophes as well, I can't, because it's difficult to distinguish between those and the start of a sentence. So, for example, my code will leave `can't` unchanged, but will turn `'hello world'` into `hello world`, and similarly will turn `'twas` into `twas`. 

```{r corpus_custom}
# remove any word containing numbers
myRemoveNumbers <- function(x) {
  gsub("\\S*[0-9]+\\S*", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemoveNumbers))

# convert smart single quotes to straight single quotes
mySingleQuote <- function(x) {
  gsub("[\x82\x91\x92]", "'", x)  # ANSI version, not Unicode version
}
blog.corpus <- tm_map(blog.corpus, content_transformer(mySingleQuote))

# custom function to remove most punctuation
myRemovePunctuation <- function(x) {
  # replace everything that isn't alphanumeric, space, apostrophe or hyphen
  gsub("[^[:alnum:][:space:]'-]", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemovePunctuation))

# deal with dashes and apostrophes within words
myDashApos <- function(x) {
  x <- gsub("--+", " ", x)
  gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myDashApos))
```

Finally, compress extra whitespace, and trim leading and trailing whitespace.

```{r corpus_clean}
# strip extra whitespace
blog.corpus <- tm_map(blog.corpus, content_transformer(stripWhitespace))
# trim leading and trailing whitespace
trim <- function(x) {
  gsub("^\\s+|\\s+$", "", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(trim))
# inspect(blog.corpus[22])
```

## Create a term-document matrix 

In general, a term-document matrix (TDM) is a mathematical matrix that describes the frequency of terms -- or essentially words -- that occur in a collection of documents (source: [Wikipedia](http://en.wikipedia.org/wiki/Document-term_matrix)). The rows of a TDM correspond to terms, and the columns correspond to the documents. (A document-term matrix has it the other way round, and is simply the transpose of the TDM.)

Each line in the blog, news, and Twitter subsamples is considered one document. The terms in the rows correspond to words found in the subsample. 

```{r tdm}
blog.tdm <- TermDocumentMatrix(blog.corpus)
```

Example of how to navigate TDM: Let's look for the term "winter", plus the 4 terms that come after alphabetically.

```{r winter}
i <- which(dimnames(blog.tdm)$Terms == "winter")
inspect(blog.tdm[i+(0:5), 1:5])
```

### Histogram to see word frequency

From the TDM, we can see which are the most frequent words. Let's look at which words appear at least 10000 times.

```{r hist}
blog.freq <- findFreqTerms(x=blog.tdm, lowfreq=10000, highfreq=Inf)
```

As expected, these are very common words. If we want to, it's possible to remove common words from the corpus. 


## end

********

#### Notes and credits:

I learned about how to use the `tm` package from these sources:

* http://www.unt.edu/rss/class/Jon/Benchmarks/TextMining_L_JDS_Jan2014.pdf

* http://www.r-bloggers.com/text-mining-the-complete-works-of-william-shakespeare/

* http://www.slideshare.net/rdatamining/text-mining-with-r-an-analysis-of-twitter-data

Regex help that I relied on: 

* http://axonflux.com/handy-regexes-for-smart-quotes

* https://books.google.com.sg/books?id=xz37q8h49iYC&pg=PT44&lpg=PT44&dq=text+mining+should+i+remove+hyphens&source=bl&ots=wHc3T7krC-&sig=qSugQYLDgmKyas3_2Vrz9ZqN21E&hl=en&sa=X&ei=5n8NVf29IcjnuQSQ1oGwBA&redir_esc=y#v=onepage&q=text%20mining%20should%20i%20remove%20hyphens&f=false
